{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Getting Started</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worum wird es in diesem Kapitel gehen? Wir werden die Kernkomponenten der NNs und Keras kennenlernen und verschiedene Ansätze zu verschiednen Arten von Problemen sehen. Beispiele werden sein:\n",
    "1. Filmreviews klassifizieren: binäre Klassifikation\n",
    "2. News nach Themen sortieren: Multiklassen-Klassifikation\n",
    "3. Die Preise von Häusern voraussagen: Regression<br>\n",
    "Die wichtigesten Objekte, aus denen das NN gebaut wird, sind die Schichten, die Daten und Ziele, die Verlustfunktion und der Optimierer:\n",
    "<img src=\"./imgs/Funktionsweise_NN.JPG\">\n",
    "Wir werden diese noch einmal kurz anschauen:<br><br>\n",
    "<b>Schichten</b><br>\n",
    "In den meisten Fällen besteht ein NN aus einem azyklischen Graphen von Schichten, welche meistens linear aufgeschichtet sind. Das bedeutet, es gibt einen einzigen Input, der zu einem einzigen Output führt. Es gibt aber mehr Architekturen, z.B. Two-Branch-Netzwerke, Multihead-Netzwerke, Inception-Blöcke, die später besprochen werden. Die Wahl der Topologie ist sehr wichtig, denn sie definiert den <i>Hypothesenraum</i>, also alle Möglichkeiten, wie die Daten dargestellt werden können.<br><br>\n",
    "<b>Verlustfunktionen und Optimierer</b><br>\n",
    "Sobald die Architektur des Netzwerks definiert ist, müssen diese beiden gewählt werden. Die Verlustfunktion ist der Wert, der während des Trainings minimiert werden musst. Sie ist eine Funktion der vorhergesagten Ziel ^Y und der echten Ziele Y. Der Optimierer bestimmt, auf welche Weise das Netzwerk aufgrund der Verlustfunktion verändert wird. Ein NN kann mehrere Verlustfunktionen besitzen, wenn es mehrere Outputs besitzt. Für übliche Probleme wie Regression oder Klassifizierung gibt es Leitlinien zur Auswahl der Verlustfunktion.<br><br>\n",
    "<b>Keras</b><br>\n",
    "Ist ein Framework für Python, das verschiedene Backends wie TensorFlow, Theano und CNTK nutzen kann. Diese können wiederrum CPU und GPUs nutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Entwicklung in Keras</h1><br>\n",
    "Die üblichen Schritte in der Entwicklung eines NN sind<br>\n",
    "1. das Definieren der Trainingsdaten und Ziele und deren Vorbereitung zur Verarbeitung,<br>\n",
    "2. das Definieren der Topologie des NN,<br>\n",
    "3. Wahl der Verlustfunktion, Optimierer und zu beobachtenden Metriken und<br>\n",
    "4. das Trainieren durch den Aufruf von <code>.fit()</code>.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential() #  Sequential: adding layers one after another\n",
    "# there is another, less useful way of doing so\n",
    "model.add(layers.Dense(32, activation = 'relu', input_shape = (784, )))\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784, )))\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "model.compile(\n",
    "    optimizer = optimizers.RMSprop(lr = 0.001),\n",
    "    loss = 'mse',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Der IMBD-Datensatz</h1><br>\n",
    "- 50 000 Reviews der Internet Movie Database, 25 000 als gut bewertet, alle weiteren als schlecht<br>\n",
    "- es ist bereits in Keras vorhanden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 15s 1us/step\n",
      "25000\n",
      "25000\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 100)\n",
    "# num_words: how many of the most commonly used words included (so that vectors are managebly small)\n",
    "print(len(test_labels))\n",
    "print(len(train_data))\n",
    "print(len(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warum wurden die Daten in Trainingsdaten und Testdaten geteilt? Weil ein Modell nicht auf den gleichen Daten trainiert und getestet werden sollte. Das liegt daran, dass \"Overfitting\" verhindert werden soll. Das bedeutet, dass das Modell die Daten auswendig lernt, das Modell also so spezifisch auf die Trainigsdaten gefittet ist, dass es keine auf weitere Daten übertragbare Abstraktion ist. Das Modell ist also zu sehr auf die Eigenheiten des vorliegenden Datensatzes angepasst. Um zu verhindern, dass die Loss Function anhand auswendig gerlernter Daten ermittelt wird, sondern die Leistungsfähigkeit des Modells auf unbekannte Daten, benutzt man getrennte Test- und Trainigsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 4, 2, 36, 2, 5, 25, 2, 43, 2, 2, 50, 2, 2, 9, 35, 2, 2, 5, 2, 4, 2, 2, 2, 2, 2, 2, 39, 4, 2, 2, 2, 17, 2, 38, 13, 2, 4, 2, 50, 16, 6, 2, 2, 19, 14, 22, 4, 2, 2, 2, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 2, 12, 8, 2, 8, 2, 5, 4, 2, 2, 16, 2, 66, 2, 33, 4, 2, 12, 16, 38, 2, 5, 25, 2, 51, 36, 2, 48, 25, 2, 33, 6, 22, 12, 2, 28, 77, 52, 5, 14, 2, 16, 82, 2, 8, 4, 2, 2, 2, 15, 2, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 7, 4, 2, 2, 13, 2, 88, 4, 2, 15, 2, 98, 32, 2, 56, 26, 2, 6, 2, 2, 18, 4, 2, 22, 21, 2, 2, 26, 2, 5, 2, 30, 2, 18, 51, 36, 28, 2, 92, 25, 2, 4, 2, 65, 16, 38, 2, 88, 12, 16, 2, 5, 16, 2, 2, 2, 32, 15, 16, 2, 19, 2, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten bestehen aus Sequenzen von Worten, die als Zahlen codiert worden sind (0 bis 9999). Die Labels, also die Ziele sind entweder 1 oder 0. 1 steht für eine gute Review und 0 für eine schlechte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vorbereitung der Daten</b><br>\n",
    "Es ist einfach möglich, Listen unterschiedlicher Länge von Integeres in ein neuronales Netz zu leiten.\n",
    "- die Inputvektoren müssen eine einheitliche Länge haben,\n",
    "- das kann mit One-Hot-Kodierung erreicht werden\n",
    "- so wird die Sequenz <code>[3, 5]</code> in einen Vektor der Länge 10 000 umgewandelt, der an den Stellen 3 und Einsen hätte und sonst Nullen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorizeData(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorizeData(train_data)\n",
    "x_test = vectorizeData(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(x_test[50])\n",
    "print(y_train[0])\n",
    "print(y_test[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der letzte Schritt bezüglich der Labels ist ebenfalls wichtig. Keras erwartet NumPy-Arrays vom Typ float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Der Bau des Netzwerks</b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu')) # Transformation auf 16 Dimension\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "# letzte Sigmoid-Schicht mit einem Neuron quetscht Werte in [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der erste Grundbaustein eines NN ist eine Schicht des Typs <code>Dense</code>.<br>\n",
    "- das bedeutet, dass jede Einheit mit jeder Einheit der Vorgängerschicht über eine Kante verbunden ist\n",
    "- der Konstruktor besitzt zwei Argumente: 16, hier die Anzahl der Einheiten in der Schicht und\n",
    "- activation, die Aktivierungsfunktion der Schicht.<br>\n",
    "\n",
    "Es gibt nun zwei wichige Fragen, wenn es zu den <code>Dense</code>-Schichten kommt:\n",
    "1. Wie viele Schichten sollten genutzt werden,\n",
    "2. wie viele versteckte Enheiten sollte die jeweilige Schicht besitzen?\n",
    "3. Welche Aktivierungsfunktionen<br>\n",
    "\"Formale\" Prinzipien zu Anzahl von Einheiten und Schichten werden später erläutert.<br><br>\n",
    "<b>Aktivierungsfunktionen</b><br>\n",
    "Eine Aktivierungsfunktion definiert den Output einer Einheit einer Schicht (Neuron) gegeben seiner Inputs. Sei y der Output, f die Aktivierungsfunktion und x der Input: $$y = f(x)$$Der Wert eines Neurons bestimmt sich durch die gewichtete Summe seiner Inputs:$$y = f(\\sum_{i=0}^{n}w_ix_i + b)$$ wenn die vorherige Schicht n Neuronen besitzt. Anhand der gewichteten Summe entscheidet das Neuron dann, welchen Wert es ausgibt. Diese Funktionen sind nötig, weil die Schicht sonst nur eine lineare Transformation der Inputdaten lernen könnte. Der Hypothesenraum der Schicht wäre die Menge aller linearer Transformationen der Inputdaten. Der Hypothesenraum ist zu klein und würde nicht von mehreren Schichten profitieren, da lineare Transformationen von linearen Transformationen den Hypothesenraum nicht erweitern, hier einige Beispiele für f.<br>\n",
    "Rectified linear unit: $$f(x)=relu(x)=max(0,x)$$\n",
    "Binary step: $$f(x)=\n",
    "\\begin{cases}\n",
    "0, x<0\\\\\n",
    "1, x\\ge 0\n",
    "\\end{cases}$$\n",
    "Sigmoid: sie ist nützlich für binäre Klassifkation als allerletztes Outputneuron, sie presst Werte zwischen 0 und 1 und kann daher als eine Wahrscheinlichkeit interpretiert werden, sodass anhand des Kriterium > 0.5 entschieden werden kann, wie die Observation klassifiziert wird.\n",
    "$$f(x)=sig(x)={{e^x}\\over{1+e^x}}$$\n",
    "So eignen sich für Regressionen am besten die linearen Aktivierungsfunktionen, für Multiklassen-Klassifizierung die Softmax und für die binäre Klassifizierung die Sigmoid am besten. Relu ist die beliebteste, wenn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nun noch den Optimierer, Verlustfunktion und Metriken festlegen\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'binary_crossentropy', # best for binary classification together with sigmoid in last neuron\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "# sie müssen nicht als Strings übergeben werden, es ist auch mäglich sie aufzurufen, zu konfigurieren und dann zu übergeben\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizers.RMSprop(lr = 0.001),\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Der Validierungsdatensatz</b><br>\n",
    "Du hast bereits deine Daten in einen Trainingsdatensatz und in einen Testdatensatz unterteilt. Nun wird der Trainingsdatensatz hälftig weiter unterteilt in einen Validierungsdatensatz und den Trainingsdatensatz. Der Validierungsdatensatz dient während des Trainings, um zu abzuschätzen, ob das Modell bessere Voraussagen tätigt als vorher: Ein Batch wird gelernt, das Modell wird mithilfe der Validierungsdaten getestet. Wenn nun mehr Werte richtig vorausgesagt worden sind als vorher, dann bewegen sich die Parameter in die richtige Richtung. Am Testdatensatz wird die Voraussagekraft außerhalb des Trainingsprozesses gemessen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 132us/step - loss: 0.6616 - acc: 0.6285 - val_loss: 0.6377 - val_acc: 0.6558\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 87us/step - loss: 0.6153 - acc: 0.6798 - val_loss: 0.6091 - val_acc: 0.6730\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.5933 - acc: 0.6903 - val_loss: 0.6079 - val_acc: 0.6696\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.5796 - acc: 0.6997 - val_loss: 0.5764 - val_acc: 0.6961\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.5739 - acc: 0.7018 - val_loss: 0.5834 - val_acc: 0.6911\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.5697 - acc: 0.7047 - val_loss: 0.5701 - val_acc: 0.7020\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.5694 - acc: 0.7049 - val_loss: 0.5712 - val_acc: 0.6997\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.5675 - acc: 0.7077 - val_loss: 0.5694 - val_acc: 0.7030\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 89us/step - loss: 0.5688 - acc: 0.7052 - val_loss: 0.5705 - val_acc: 0.7019\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 87us/step - loss: 0.5663 - acc: 0.7081 - val_loss: 0.5749 - val_acc: 0.6952\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 89us/step - loss: 0.5669 - acc: 0.7084 - val_loss: 0.5730 - val_acc: 0.6950\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 89us/step - loss: 0.5661 - acc: 0.7082 - val_loss: 0.5903 - val_acc: 0.6825\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 92us/step - loss: 0.5669 - acc: 0.7039 - val_loss: 0.5675 - val_acc: 0.7028\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.5649 - acc: 0.7049 - val_loss: 0.5691 - val_acc: 0.7008\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 92us/step - loss: 0.5643 - acc: 0.7054 - val_loss: 0.5679 - val_acc: 0.7019\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 89us/step - loss: 0.5639 - acc: 0.7071 - val_loss: 0.5683 - val_acc: 0.7002\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.5632 - acc: 0.7065 - val_loss: 0.5781 - val_acc: 0.6966\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.5630 - acc: 0.7082 - val_loss: 0.5751 - val_acc: 0.6953\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.5622 - acc: 0.7075 - val_loss: 0.5734 - val_acc: 0.6959\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.5606 - acc: 0.7081 - val_loss: 0.5769 - val_acc: 0.6976\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[:10000] # Beo 9999 und davor\n",
    "partial_x_train = x_train[10000:] # 10000 und danach\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "history = model.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs = 20,\n",
    "    batch_size = 512,\n",
    "    validation_data = (x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit() gibt ein Objekts des Typs History zurück, das Metriken über den Verlauf des Trainings hinweg enthält. Dieser ist ein Dictionary/assoziatives Array, in dem jedes Element Daten über die Metriken in jeder Trainigsepoche haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
      "[0.6377085701942444, 0.6090554194450378, 0.6079496955871582, 0.5763984783172608, 0.5833783117294311, 0.5701023338317871, 0.5712036338806152, 0.5693908110618592, 0.5705283778190613, 0.5749170949935913, 0.5729542663574219, 0.5902904415130615, 0.5675434779167176, 0.569089871788025, 0.5679305442810059, 0.5682731781959534, 0.578122270488739, 0.5751478478431702, 0.5734334106445312, 0.5769134291648865]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "historyDict = history.history\n",
    "print(historyDict.keys())\n",
    "print(historyDict['val_loss'])\n",
    "print(len(historyDict['val_loss'])) # 20 Epochen, 20 Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "# HIER WEITER\n",
    "# Fragen: \n",
    "    # woran wird der Loss ermittelt? Validationsset\n",
    "    # genau Funktionen der drei Sets\n",
    "lossVals = historyDict['loss']\n",
    "accurav\n",
    "epochs = range(1, len(historyDict['val_loss']) + 1) # 1 bis 20\n",
    "# plotte die Akkuratheit, bo heißt blue dot\n",
    "plot.plot(epochs, loss_values, 'bo', label =' Training loss')\n",
    "# plotte die den Wert der Verlustfunktion, b heißt blue\n",
    "plot.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plot.title('Training and validation loss')\n",
    "plot.xlabel('Epochs')\n",
    "plot.ylabel('Loss')\n",
    "plot.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
