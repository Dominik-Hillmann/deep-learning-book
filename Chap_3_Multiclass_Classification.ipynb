{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multiclass Classification</h1><br>\n",
    "Es gibt dabei single-lable multiclass classification, bei der eine Beobachtung nur einer Klasse zugeordnet werden kann. Daraus folgt multi-lable ..., bei der eine Beobachtung zu mehreren Klassen gleichzeitig gehören kann.<br><br>\n",
    "<b>Der Reuters-Datensatz</b><br>\n",
    "Textklassifikation: kurze Nachrichten mit dem dazugehörigen Thema. Es gibt 46 Themen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)\n",
    "# es gilt wieder, dass jede Beobachtung nur aus den 10 000 häufigsten Wörtern besteht\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kann man wie bei der binären Klassifikation Daten in natürliche Sprache zurückumwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531\n",
      "? oper shr 40 cts vs 30 cts oper net 1 364 000 vs 1 025 000 revs 14 7 mln vs 11 0 mln avg shrs 3 372 970 vs 3 425 400 year oper shr 86 cts vs 32 cts oper net 2 925 000 vs 1 109 000 revs 43 0 mln vs 35 7 mln avg shrs 3 383 651 vs 3 418 594 note year ago periods exclude extraordinary gain of 1 1 mln dlrs or 31 cts shr includes gains of 988 000 dlrs vs one mln dlrs in qtr and 2 2 mln dlrs vs 1 1 mln dlrs in year from tax loss carryforwards reuter 3\n"
     ]
    }
   ],
   "source": [
    "index = reuters.get_word_index()\n",
    "print(index['car']) # assziativer Array, Wort -> Zahl 'Dictionary' in Python\n",
    "# index.items() wandelt ein dict in Paare von zugeordneten Werten um\n",
    "# und dict() wandelt Tupel wieder in dicts um\n",
    "index_reversed = dict([(num, word) for (word, num) in index.items()])\n",
    "# hat einen Offset von 3, weil die ersten 3 für \"padding\", \"start\" und \"unknown\" stehen\n",
    "print(' '.join([index_reversed.get(i - 3, '?') for i in train_data[28]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun geht es wieder ans Umwandeln der Daten in One-Hot-Kodierung für das NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12] \n",
      "\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])\n",
      " list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12])\n",
      " list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12])\n",
      " ...\n",
      " list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12])\n",
      " list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12])\n",
      " list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hottify(newswire, num_classes):\n",
    "    results = np.zeros((1, num_classes)).flatten()    \n",
    "    for num in newswire:\n",
    "        results[num] = 1        \n",
    "    return results\n",
    "\n",
    "news = train_data[0]\n",
    "print(news, '\\n')\n",
    "print(one_hottify(news, 10000))\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2163, 317, 65, 131, 1462, 23, 768, 1225, 2, 7, 316, 5, 10, 3851, 1017, 97, 2201, 267, 2093, 248, 705, 9, 387, 262, 2, 4805, 118, 10, 163, 483, 36, 118, 4, 978, 427, 691, 491, 8140, 303, 522, 144, 34, 10, 2783, 5, 1236, 10, 1026, 24, 41, 8244, 4, 816, 168, 7372, 184, 75, 163, 126, 5, 4, 3579, 2164, 4805, 34, 3051, 6, 10, 1031, 515, 106, 909, 5, 65, 4274, 4805, 8, 144, 62, 7854, 5, 2, 696, 3851, 8034, 6, 65, 182, 4274, 64, 875, 548, 336, 7, 329, 206, 405, 439, 4, 825, 7, 4, 1142, 3813, 317, 6227, 101, 21, 2, 2552, 2988, 65, 1523, 254, 397, 5767, 6, 401, 20, 22, 193, 20, 9, 4, 5414, 5, 6227, 21, 4886, 65, 292, 54, 397, 36, 8, 4, 182, 250, 4, 8140, 40, 85, 2629, 13, 4, 3984, 5, 4169, 4274, 21, 65, 7, 809, 6, 842, 2247, 13, 9370, 112, 10, 7556, 5, 1490, 87, 9, 2163, 1577, 456, 691, 411, 184, 6223, 1381, 6, 1144, 226, 8211, 2, 6, 42, 414, 2828, 4274, 6476, 268, 1648, 8799, 6, 2563, 405, 6771, 5, 2, 8034, 984, 5, 6272, 756, 50, 2069, 106, 995, 389, 717, 9, 91, 325, 2069, 10, 1716, 5, 2512, 43, 6, 432, 2210, 201, 55, 819, 6, 391, 7, 181, 714, 4805, 8, 36, 8, 4, 106, 299, 45, 2466, 6, 455, 25, 3759, 9215, 4274, 2, 998, 189, 254, 1677, 7, 807, 6, 182, 1343, 6, 837, 137, 691, 1741, 7, 10, 630, 654, 6, 30, 2, 43, 384, 257, 2, 23, 10, 131, 5, 279, 20, 923, 6, 2237, 5261, 1304, 714, 267, 21, 454, 92, 10, 608, 101, 5, 810, 40, 85, 1980, 13, 2210, 1640, 8140, 40, 8, 16, 23, 45, 10, 3853, 74, 267, 131, 161, 691, 2, 23, 923, 6, 1633, 1640, 4805, 8, 4, 534, 45, 6, 455, 2, 34, 267, 95, 97, 1866, 21, 4, 816, 3515, 6, 2223, 4, 2, 9, 847, 5, 4, 73, 6476, 787, 24, 181, 1118, 57, 414, 85, 731, 21, 4, 1053, 2982, 1001, 17, 12] \n",
      "\n",
      "[0. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "one_hot_train_data = [one_hottify(news, 10000) for news in train_data]\n",
    "one_hot_test_data = [one_hottify(news, 10000) for news in test_data]\n",
    "print(train_data[29], '\\n')\n",
    "print(one_hot_train_data[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da es sich um ein Multiklassen-Problem handelt, müssen die Labels auch erst in One-Hot umgewandelt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 0\n",
      "3 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "max_num = max(train_labels)\n",
    "min_num = min(train_labels)\n",
    "print(max_num, min_num) # Es gibt 46 Klassen\n",
    "max_classes = max_num + 1\n",
    "\n",
    "def label_one_hottify(num, max_classes):\n",
    "    # has to deal with single numbers, not arrays\n",
    "    res = np.zeros((1, max_classes)).flatten()\n",
    "    res[num] = 1\n",
    "    return res\n",
    "\n",
    "one_hot_train_labels = [label_one_hottify(label, max_classes) for label in train_labels]\n",
    "one_hot_test_labels = [label_one_hottify(label, max_classes) for label in test_labels]\n",
    "print(train_labels[28], one_hot_train_labels[28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bau des Netzwerks</b><br>\n",
    "Es wichtig zu verstehen, dass jede Schicht ihre Informationen aus der vorangegangenen Schicht bezieht. Verlorengegangene Information ist nicht wiederherstellbar. Diese kann verloren gehen, indem z.B. eine Schicht weniger Knoten hat als die Outputschicht. Das heißt, diese Schicht stellt die Information des Inputs in weniger Knoten dar als die des Outputs und komprimiert Information über das geforderte Maß der letzten Schicht hinaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(max_classes, activation = 'softmax'))\n",
    "# softmax in the context of multiclass means that it will output a probability \n",
    "# distribution over all 46 classes\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bereits beim MNIST-Datensatz, legen wir uns auch hier Validierungssatz bei Seite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n",
      "46\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 87us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 64us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0791 - val_acc: 0.7590\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 73us/step - loss: 0.7034 - acc: 0.8470 - val_loss: 0.9842 - val_acc: 0.7820\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 70us/step - loss: 0.5665 - acc: 0.8800 - val_loss: 0.9416 - val_acc: 0.8040\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.4582 - acc: 0.9048 - val_loss: 0.9075 - val_acc: 0.8020\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9353 - val_acc: 0.7900\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 0.3033 - acc: 0.9311 - val_loss: 0.8917 - val_acc: 0.8090\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 0.2536 - acc: 0.9415 - val_loss: 0.9065 - val_acc: 0.8100\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 69us/step - loss: 0.2187 - acc: 0.9468 - val_loss: 0.9177 - val_acc: 0.8140\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.1874 - acc: 0.9508 - val_loss: 0.9026 - val_acc: 0.8140\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1700 - acc: 0.9523 - val_loss: 0.9327 - val_acc: 0.8090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1534 - acc: 0.9558 - val_loss: 0.9687 - val_acc: 0.8070\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1390 - acc: 0.9560 - val_loss: 0.9685 - val_acc: 0.8130\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 68us/step - loss: 0.1313 - acc: 0.9568 - val_loss: 1.0250 - val_acc: 0.8030\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1217 - acc: 0.9577 - val_loss: 1.0280 - val_acc: 0.7940\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 70us/step - loss: 0.1196 - acc: 0.9578 - val_loss: 1.0422 - val_acc: 0.8070\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1136 - acc: 0.9593 - val_loss: 1.0945 - val_acc: 0.7970\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1111 - acc: 0.9593 - val_loss: 1.0694 - val_acc: 0.8020\n"
     ]
    }
   ],
   "source": [
    "one_hot_val_data = one_hot_train_data[:1000]\n",
    "one_hot_val_labels = one_hot_train_labels[:1000]\n",
    "partial_one_hot_train_data = one_hot_train_data[1000:]\n",
    "partial_one_hot_train_labels = one_hot_train_labels[1000:]\n",
    "print(len(one_hot_val_data), len(one_hot_val_labels))\n",
    "print(len(one_hot_val_labels[0]))\n",
    "\n",
    "# nicht vergessen, dass Keras Numpy-Arrays haben möchte\n",
    "history = model.fit(\n",
    "    np.array(partial_one_hot_train_data),\n",
    "    np.array(partial_one_hot_train_labels),\n",
    "    epochs = 20,\n",
    "    batch_size = 512,\n",
    "    validation_data = (\n",
    "        np.array(one_hot_val_data), \n",
    "        np.array(one_hot_val_labels)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.53220713918175, 1.4452012063148056, 1.0952716777570204, 0.8696630689710102, 0.7033508893870615, 0.566495376522157, 0.4581522696584545, 0.36952803996504774, 0.3033050218939154, 0.2535927318273646, 0.21870324979283046, 0.18739201861033014, 0.17004525053250225, 0.15339488049806194, 0.13901008078252067, 0.1313314037502514, 0.12172376538459534, 0.11959134567966738, 0.11355109715171996, 0.11110593108224617] [0.4954898522868547, 0.6879228267924166, 0.7650964676930593, 0.8164620390663586, 0.8470308194181968, 0.8799799540920444, 0.9047857683513195, 0.9230769229574449, 0.9310949645344699, 0.941493361030895, 0.9467551981079582, 0.9507642197776217, 0.9522676025826481, 0.9557754942660521, 0.9560260586169871, 0.9567777492354251, 0.9576547239036436, 0.9577800044138848, 0.9592833870247592, 0.9592833870247592]\n"
     ]
    }
   ],
   "source": [
    "print(history.history['loss'], history.history['acc'])\n",
    "# reached ~ 95% accuracy, but starts overfitting at epoch 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's test the overfitted network on the test data and see how it performs.<br><br>\n",
    "<b>How to perform predictions on inputs with Keras</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.11612072e-03 2.43878854e-03 4.50072926e-04 2.58446541e-02\n",
      " 1.03574258e-03 2.35793251e-03 4.97838164e-06 1.40638952e-03\n",
      " 6.41598599e-04 1.37327152e-05 8.83599758e-01 5.78736560e-03\n",
      " 3.33731901e-03 1.50280639e-05 3.17545753e-04 4.62839562e-05\n",
      " 1.38985820e-03 2.12274652e-04 1.01340178e-04 1.99131668e-02\n",
      " 5.14019572e-04 2.26484590e-05 2.58344645e-03 5.24944579e-03\n",
      " 1.79268565e-04 4.04455961e-04 2.39229202e-03 9.38731027e-05\n",
      " 4.10840876e-04 7.12614710e-05 9.12602718e-06 3.75240430e-04\n",
      " 3.42903804e-05 1.37605075e-05 2.91637261e-04 3.98208795e-05\n",
      " 3.61901186e-02 1.00982063e-04 3.13512690e-04 3.02752978e-05\n",
      " 5.17625085e-05 3.24852823e-04 1.09544868e-04 8.66560822e-06\n",
      " 2.86654813e-06 1.51989254e-04]\n",
      "0.88359976 10\n",
      "3\n",
      "0.9999999489305083\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(np.array(one_hot_test_data))\n",
    "print(predictions[28]) # Wahrscheinlichkeiten über die 46 Klassen\n",
    "print(max(predictions[28]), np.argmax(predictions[28]))\n",
    "print(np.argmax(one_hot_test_labels))\n",
    "# ausserdem\n",
    "print(sum(predictions[28])) # ~ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die höchste Wahrscheinlichkeit gilt Klasse 9 mit ~90%. Was ist der echte Werte der Testbeobachtung 28? Leider Klasse 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Was du hieraus mitnehmen solltest</b><br><br>\n",
    "Wenn du Daten in N Klassen einordnen möchtest, braucht die Outputschicht N Neuronen. Diese sollte eine Softmax-Funktion für eine Wahrscheinlichkeitsverteilung über die Klassen nutzen. Du solltest fast immer die Categorical Crossentropy für solche Probleme nutzen, da sie den Abstand der Verteilung und des Labels (als One-Hot) minimiert. Vermeide Informationsflaschhälse in Form von Schichten mit weniger Neuronen als Klassen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
