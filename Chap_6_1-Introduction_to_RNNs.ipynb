{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks work best with any kind of sequence data. This can be for example time series prediction, sequence-to-sequence learning, sentiment analysis.\n",
    "\n",
    "## Words to Numbers\n",
    "Alle Modelle brauchen eine numerische Darstellung der Inputdaten, was sich auf viele Arten machen lässt.\n",
    "Zum einen lassen sich Worte, aber auch Zeichen und n-grams in Zahlen darstellen.\n",
    "n-grams sind Gruppen $n$ aufeinanderfolgender Worte. Beispielsweise wird aus `The cat sat on the mat` folgende bag-of-words genannte Gruppe: `{\"The\", \"The cat\", \"cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}`. Für eine bag-of-words aus 1-grams wäre dies: `{\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"}`.\n",
    "Diese Methode ist aber eher für für flache Lernverfahren geeignet, da das Konzept der bag-of-words keine Reihenfolge kennt (`{}`).\n",
    "Dieser Prozess der Aufteilung eines Texts in kleinere Bestandteile wird Tokenization genannt.\n",
    "\n",
    "Wie lässt lassen sie sich nun in Zahlen umwandeln?\n",
    "### One-Hot Encoding\n",
    "Zähle, welche die $n$ meisten vorkommenden Wörter im Text sind. Baue jeden Subbestandteil einen Vektor mit $n$ Elementen. Eine $1$ an Stelle $i$ des Vektors bedeutet, dass das am $i$-ten meist vorkommende Wort in diesem Bestandteil vorhanden ist.\n",
    "### Word Embeddings\n",
    "Sind hochdimensionale Vektoren, die eine sematische Nähe der Worte zueinander kodieren, so sollten die Vektoren beispielsweise folgende Beziehung darstellen können: `embedding(\"king\") + embedding(\"female\") == embedding(\"queen\")`. Der Vorteil zum One-Hot Encoding ist, dass die Dichte an Information viel größer ist, weil der Großteil der One-Hot-Vektoren aus $0$ besteht.\n",
    "\n",
    "Diese Vektoren können gelernt werden, z.B. als Schicht eines NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding = Embedding(1000, 64) # (maximum number of tokens as input, dimensionality of vector)\n",
    "# The input to the layer is a tensor shaped (number of samples in batch, length of these sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genauso, wie sich vortrainierte Convolution-Schichten nutzen lassen, die auf ähnlichen Problemen trainiert wurden, da diese ebenso nützliche low-level-Strukturen gefunden haben. Bekannte Beispiele sind `Word2Vec` und `GloVe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
