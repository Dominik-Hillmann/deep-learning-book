{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks work best with any kind of sequence data. This can be for example time series prediction, sequence-to-sequence learning, sentiment analysis.\n",
    "\n",
    "## Words to Numbers\n",
    "Alle Modelle brauchen eine numerische Darstellung der Inputdaten, was sich auf viele Arten machen lässt.\n",
    "Zum einen lassen sich Worte, aber auch Zeichen und n-grams in Zahlen darstellen.\n",
    "n-grams sind Gruppen $n$ aufeinanderfolgender Worte. Beispielsweise wird aus `The cat sat on the mat` folgende bag-of-words genannte Gruppe: `{\"The\", \"The cat\", \"cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}`. Für eine bag-of-words aus 1-grams wäre dies: `{\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"}`.\n",
    "Diese Methode ist aber eher für für flache Lernverfahren geeignet, da das Konzept der bag-of-words keine Reihenfolge kennt (`{}`).\n",
    "Dieser Prozess der Aufteilung eines Texts in kleinere Bestandteile wird Tokenization genannt.\n",
    "\n",
    "Wie lässt lassen sie sich nun in Zahlen umwandeln?\n",
    "### One-Hot Encoding\n",
    "Zähle, welche die $n$ meisten vorkommenden Wörter im Text sind. Baue jeden Subbestandteil einen Vektor mit $n$ Elementen. Eine $1$ an Stelle $i$ des Vektors bedeutet, dass das am $i$-ten meist vorkommende Wort in diesem Bestandteil vorhanden ist.\n",
    "### Word Embeddings\n",
    "Sind hochdimensionale Vektoren, die eine sematische Nähe der Worte zueinander kodieren, so sollten die Vektoren beispielsweise folgende Beziehung darstellen können: `embedding(\"king\") + embedding(\"female\") == embedding(\"queen\")`. Der Vorteil zum One-Hot Encoding ist, dass die Dichte an Information viel größer ist, weil der Großteil der One-Hot-Vektoren aus $0$ besteht.\n",
    "\n",
    "Diese Vektoren können gelernt werden, z.B. als Schicht eines NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding = Embedding(1000, 64) # (maximum number of tokens as input, dimensionality of vector)\n",
    "# The input to the layer is a tensor shaped (number of samples in batch, length of these sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genauso, wie sich vortrainierte Convolution-Schichten nutzen lassen, die auf ähnlichen Problemen trainiert wurden, da diese ebenso nützliche low-level-Strukturen gefunden haben. Bekannte Beispiele sind `Word2Vec` und `GloVe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weitere spezielle Schichten von RNNs\n",
    "\n",
    "Wenn der Mensch einen Text liest, so tut er das, indem einzelne Wortgruppen im Kontext der vorangegangenen Worte gelesen werden. Das wird in RNNs so modelliert, dass der vorangegangene Output eines Teils der gesamten Inputsequenz als Input des nächsten Teils verwendet wird. Die gesamte Sequenz wird also als Loop ihrer Bestandteile verarbeitet. Sei im folgenden Code der Input ein Tensor der Form `(Zeitschritte, Input Features)`, also die gesammte Inputsequenz mit `Input Features` Elementen wird in `Zeitschritte` zeitlich aufeinander aufbauende Schritte unterteilt. In Pseudocode die Mechanik: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = 0\n",
    "\n",
    "for input_t in input_sequence: # zeitlich getrennte Sequenzen in Gesamtsquenz\n",
    "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "    state_t = output_t\n",
    "# W und U sind durch Backpropagation modifizierte Gewichtsmatrizen, b der Bias    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Output der vorangegangenen Teilsequenz wird mit einem eigenen Gewicht in den Output der jetzigen Sequenz einbezogen.\n",
    "\n",
    "### Eine einfacht Recurrent Layer\n",
    "Auf diesem Prinzip beruht auch die `SimpleRNN`-Schicht in Keras. Allerdings nimmt diese nicht eine einzelne Gesamtsequenz entgegen, sondern ein Batch dieser. Der Inputtensor hat die Form `(batch_size, timesteps, input_features)`. Der Output wiederrum `(bactch_size, output_features)`.\n",
    "\n",
    "Wenn es um die Frage geht, ob _alle_ bisher erstellten Output in den nächsten Input einbezogen werden sollen, wird `return_sequences = True` genommen. Ansonsten wird nur der Output der letzten Teilsequenz genutzt.\n",
    "\n",
    "Wenn mehrere `SimpleRNN`-Schichten hintereinandergeschaltet werden, dann muss jede außer die letzte mit `return_suequences = True` versehen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32)) # max num tokens, dim output vec\n",
    "model.add(SimpleRNN(32)) # Gesamtsequenzen des Batchs hat 32 Elemente, durch Embedding festgelegt\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, None, 32)          2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(10000, 32)) # max num tokens, dim output vec\n",
    "model2.add(SimpleRNN(32, return_sequences = True)) # Gesamtsequenzen des Batchs hat 32 Elemente, durch Embedding festgelegt\n",
    "model2.summary()\n",
    "# Hier ist nun zu sehen, dass eine zusätzliche Dimension hinzugefügt wurde, da das RNN nun mehrere Outputs für alle Teil\n",
    "# -sequenzen einer Gesamtsequenz weitergibt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "LSTM steht für "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
