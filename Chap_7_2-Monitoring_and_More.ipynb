{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Monitoring\n",
    "Da nach dem Anstoß zum Training recht wenig Kontrolle über den Prozess. Hier werden Funktionen vorgestellt, diesen Prozess besser zu beobachten und zu steuern.\n",
    "\n",
    "Bisher wurde der Trainingsprozess so vorgestellt, dass so viele Epochen trainiert werden sollte, bis das Modell anfängt zu overfitten. Wann der Trainingsprozess endet lässt sich auch über Callbacks steuern.\n",
    "\n",
    "Callbacks sind Objekte, die bestimmte Funktionen implementieren und anderen Objekten übergeben werden, um sie situativ aufzurufen und so dem User zu ermöglichen, einen gegebenen Prozess anzupassen, indem er verschiedene Callbacks verwendet.\n",
    "\n",
    "Nachfolgend wird gezeigt, wie sie verwendet werden und eine paar Callbacks vorgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping( # unterbricht Training, wenn sich Metrik nicht mehr verbessert für [patience] Epochen\n",
    "        monitor = 'acc',\n",
    "        patience = 1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint( # Saves weights every epoch\n",
    "        filepath = 'my_model.h5',\n",
    "        monitor = 'val_loss',\n",
    "        save_best_only = True\n",
    "    ),\n",
    "    callbacks.ReduceROnPleateau( # reduziert Lernrate auf 10%\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 10\n",
    "    )\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['acc'] # Sollte hier sein, wenn durch Callback beobachtet\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X, y,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    callback = callbacks_list,\n",
    "    validation_data = (X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es lassen sich auch komplett eigene Callbacks definieren, was hier vorgestellt wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback): # erweitere gegebene Keras-Klasse\n",
    "    def set_model(self, model):\n",
    "        self.model = model # Called before training to inform callback what model it will be called by\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        self.activations_model = keras.models.Model(model.input, layer_outputs)\n",
    "        \n",
    "    # alle implementierbaren Methoden sind selbsterklärend und hier aufgelistet\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        pass\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def on_batch_begin(self, epoch, logs = None):\n",
    "        pass\n",
    "    def on_batch_end(self, epoch, logs = None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        pass\n",
    "    def on_train_end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsvisualisierung mit TensorBoard\n",
    "TensorBoard kommt mit TensorFlow und ist browserbasiert. Es stellt eine große Menge an Visualisierungsmöglichkeiten für das Modell und sein Training bereit. Wir demonstrieren an dem einfachen IMDB-Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 16:39:14.030402 140513352116032 deprecation.py:323] From /home/dominik/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 30s 1ms/sample - loss: 0.5940 - acc: 0.6844 - val_loss: 0.6633 - val_acc: 0.7452\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.4284 - acc: 0.8535 - val_loss: 0.4718 - val_acc: 0.8396\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.3774 - acc: 0.8832 - val_loss: 0.5085 - val_acc: 0.8394\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.3193 - acc: 0.9021 - val_loss: 0.5004 - val_acc: 0.8682\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 30s 1ms/sample - loss: 0.2861 - acc: 0.9194 - val_loss: 1.2742 - val_acc: 0.7562\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.2444 - acc: 0.9372 - val_loss: 0.6013 - val_acc: 0.8756\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.2041 - acc: 0.9556 - val_loss: 0.7783 - val_acc: 0.8554\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.1773 - acc: 0.9658 - val_loss: 0.7439 - val_acc: 0.8724\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 34s 2ms/sample - loss: 0.1515 - acc: 0.9765 - val_loss: 0.8715 - val_acc: 0.8654\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 41s 2ms/sample - loss: 0.1213 - acc: 0.9855 - val_loss: 0.9931 - val_acc: 0.8632\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.1123 - acc: 0.9872 - val_loss: 1.0292 - val_acc: 0.8652\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.1018 - acc: 0.9894 - val_loss: 1.0173 - val_acc: 0.8712\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.1005 - acc: 0.9898 - val_loss: 1.0630 - val_acc: 0.8728\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 30s 2ms/sample - loss: 0.0941 - acc: 0.9916 - val_loss: 1.1121 - val_acc: 0.8710\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.0933 - acc: 0.9919 - val_loss: 1.1236 - val_acc: 0.8708\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.1022 - acc: 0.9904 - val_loss: 1.0993 - val_acc: 0.8726\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 28s 1ms/sample - loss: 0.0914 - acc: 0.9916 - val_loss: 1.2960 - val_acc: 0.8632\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.0881 - acc: 0.9923 - val_loss: 1.1884 - val_acc: 0.8698\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.0897 - acc: 0.9926 - val_loss: 1.1828 - val_acc: 0.8672\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.0918 - acc: 0.9923 - val_loss: 1.2179 - val_acc: 0.8692\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "max_features = 2000\n",
    "max_len = 500\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = imdb.load_data(num_words = max_features)\n",
    "train_X = sequence.pad_sequences(train_X, maxlen = max_len)\n",
    "test_X = sequence.pad_sequences(test_X, maxlen = max_len)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    max_features, 128,\n",
    "    input_length = max_len,\n",
    "    name = 'embed'\n",
    "))\n",
    "model.add(layers.Conv1D(32, 7, activation = 'relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation = 'relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['acc']\n",
    ")\n",
    "\n",
    "# wichtig, es sollte ein Ordner für das Monitoring angelegt werden\n",
    "# Nun Tensorboard callback:\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir = 'training_monitoring',\n",
    "        histogram_freq = 1,\n",
    "        embeddings_freq = 1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_X, train_y,\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = callbacks\n",
    ")\n",
    "\n",
    "# start TensorBoard server with tensorboard --logdir=training_monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file = 'model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Performance Enhancing Methods\n",
    "### Batch Normalization\n",
    "Sind viele Methoden, die die Beobachtungen einander angleichen wollen. Zum Beispiel die Normalisierung bei Normalverteilung, sodass $\\mu = 0$ und $\\sigma = 1$.\n",
    "Normalisierung sollte auch zwischen den Schichten stattfinden, da es keine Garantie gibt, dass Daten, die das Ergebnis einer Schicht und normalisierten Inputdaten sind auch normal sind. Daher gibt es eine Schicht für diese Aufgabe: `BatchNormalization`. Diese Schicht hilft außerdem, die Updates der Backpropagation weiterzutragen, was ein tieferes Netzwerk ermöglicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "model.add(BatchNormalization())\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Renormalization\n",
    "Ist eine Weiterentwicklung, die bisher keine Nachteile aufweist. Es gibt auch Entwicklungen, die die Daten über das gesamte Netzwerk normal halten, indem spezielle Aktivierungsfunktionen und Initialisierungen genutzt werden.\n",
    "\n",
    "### Depthwise Seperable Convolution\n",
    "Ist eine Möglichkeit, die Convolutionschichten, schneller, leichter und präziser zu machen und kann mit dem `SeperableConv2D`-Konstruktor genutzt werden. Kurzgesagt nutzt sie die Convolution-Operation auf jedem Kanal des Bildes seperat.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "Hyperparameter bezeichnet Dinge wie die Tiefe des Netzwerks, ob `BatchNormalization`-Schichten genutzt werden, oder welche Aktivierungsfunktion eingsetzt wird. In der Praxis bauen Machine Learning Engineers mit der Zeit eine Intuition für diese Entscheidungen auf, aber wie kann hier ein Anfänger begründete Entscheidungen treffen?\n",
    "Der typische Prozess läuft meist wiefolgt ab:\n",
    "1. Wahl der anzupassenden Hyperparameter\n",
    "2. Bauen des Modells\n",
    "3. Fitting des Modells und Messen der Performanz anhand der Validierungsdaten\n",
    "4. Wähle Hyperparameter neu\n",
    "5. Wiederholde, bis eine gute Auswahl an Modellen gefunden ist\n",
    "6. Teste die besten Modelle anhand der Testdaten.\n",
    "\n",
    "Viele andere Möglichkeiten sind bekannt: genetische Algorithmen, Bayesianische Optimierung, Zufall, etc.\n",
    "Hier ist allerdings wichtig, dass Schritt 4 in Abhängigkeit der vorangegangen Parameter gewählt wird. Denn bewegt sich die Performanz in eine gute Richtung, ist das Optimum vielleicht in dieser Richtung zu finden.\n",
    "Gute Ansätze für automatische Wahl von Hyperparametern werden in den Repos von `Hyperopt` und `Hyperas` implementiert.\n",
    "\n",
    "### Model Ensembling\n",
    "Das ist eine Strategie, bei der viele Modelle eine gepoolte Voraussage treffen und ein in Wettbewerben sehr erfolgreicher Ansatz ist. Hier sind mehrere Ansätze den Durchschnitt einer Voraussage zu wählen oder ein gewichtetes Mittel, wobei die Gewichte von der Performanz auf die Validierungsdaten abhängig sind. Der Schlüssel hier die Unterschiedlichkeit der Modelle, während sie trotzdem einzeln gute Ergebnisse liefern. So können sie verschiedene Machine-Learning-Algorithmen sein oder komplett verschiedene Architekturen von Deep Learning-Modellen. Baumbasierte Methoden und NNs scheinen hierbei besonders gut zusammen zu funktionieren."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
