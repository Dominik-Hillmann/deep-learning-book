{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Das Beispiel-Netzwerk</h1><br>\n",
    "Das zu lösende Probleme soll die Einordnung von 28px mal 28px großen Bildern von Zahlen in Schreibschrift in die Kategorien 0 bis 9 sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "5\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "print(train_labels[0])\n",
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird ein Netzwerk gebaut, dem die Bilder train_images und die Zuordnung train_labels. Daraus entwickelt das Netzwerk Zuordnungen, die anhand des Vergleichs des errechneten Outputs von test_images[i] mit test_labels[i] verglichen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation = 'relu', input_shape=(28 * 28, )))\n",
    "network.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir bauen ein Netzwerk, das in der ersten Schicht 512 Einheiten besitzt und Daten der Größe <code>(28 * 28, )</code> erwartet. <code>Dense</code> heißt, dass jede Einheit dieser Schicht mit allen Einheiten der vorhergehenden Schicht verbunden ist. Die letzte Schicht besitzt eine <code>softmax</code> Aktivierungsfunktion, was bedeutet, dass sie 10 Wahrscheinlichkeiten für die 10 Kategorien zurückgeben wird.<br>\n",
    "Nun müssen noch drei Dinge definiert werden, bevor das Netzwerk die Arbeite aufnehmen kann:\n",
    "1. Die Loss-Function: wie das Netzwerk Erfolg beim Vergleich mit den Testdaten misst\n",
    "2. Der Optimierer, also wie sich das Netzwerk optimiert, basierend auf der Loss-Function und den Daten\n",
    "3. Die zu beobachtenden Metriken während des Trainings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor das Netzwerk trainiert werden kann, müssen die Daten in Vektoren aus Nullen und Einsen umgewandelt werden. Unsere Trainingsbilder haben die Form <code>(60000, 28, 28)</code>, also 60000 Bilder mit 28 mal 28 Pixeln, wobei jeder Pixel durch eine Zahl zwischen 0 und 255 dargestellt wird, also wie schwarz/weiß dieser ist. Diese werden nun in <code>float32</code>-Werte <i>zwischen</i> 0 und 1 umgewandelt in der Form <code>(60000, 28 * 28)</code> (60000 Beobachtungen mit einem 1D-Array aus Werten zwischen 0 und 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2544 - acc: 0.9268\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1033 - acc: 0.9696\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0683 - acc: 0.9795\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0503 - acc: 0.9848\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0373 - acc: 0.9886\n",
      "10000/10000 [==============================] - 0s 28us/step\n",
      "Accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28)) # neue Form mit flachen Arrays\n",
    "train_images = train_images.astype('float32') / 255 # float32 auf 0...1 normiert\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# labels umwandeln\n",
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# alles ist getan, das Netzwerk kann traniert werden\n",
    "network.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs = 5,\n",
    "    batch_size = 128\n",
    ")\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Datenrepräsentationen</h1><br>\n",
    "Tensoren sind die generelle Bezeichnung für Vektoren, Matrizen, Skalare, etc. Sie sind die in NN Netzwerk am meisten genutzte Datenrepräsentation.<br><br>\n",
    "<b>Skalare - 0D</b><br>\n",
    "Das sind Tensoren mit nur einer Zahl. In Numpy können float32 oder float64-Zahlen Skalare sein.<br><br>\n",
    "<b>Vektoren - 1D</b><br>\n",
    "Vektoren haben exakt eine Dimension, entlang der Zahlen angeordnet sind.<br><br>\n",
    "<b>Matrizen - 2D</b><br>\n",
    "Haben zwei Dimensionen, genannt Zeilen und Spalten. Beispiele für Daten aus der realen Welt, die als Matrix angeordnet sind, sind Daten, bei den für n Beobachtungen p Variablen augezeichnet worden sind: <code>(Beobachtungen, Variablen)</code>.<br><br>\n",
    "<b>3+D</b><br>\n",
    "3D-Tensoren kann man z.B. als Anordnung in einem Würfel interpretieren. Ein Beispiel für 3D-Daten sind Zeitreihendaten, wo wieder für n Beobachtungen p Variablen aufgezeichnet worden sind, dies aber zu T Zeitpunkten wiederholt wurde: <code>(Beobachtungen, Variablen, Zeitpunkte)</code>.<br>\n",
    "4D - Bilder: <code>(Beobachtungen, Px in x-Dimension, Px in y-Dimension, Kanäle (Farben))</code>.<br>\n",
    "5D - Videos: <code>Beobachtungen, Frame, Px in x-Dimension, Px in y-Dimension, Kanäle (Farben))</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "0\n",
      "[12  5  2]\n",
      "[[1. 2. 4.]\n",
      " [5. 2. 5.]\n",
      " [6. 9. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "print(x) # Skalar\n",
    "print(x.ndim) # so kann man sich zeigen lassen, wie viele Dimensionen Tensor hat\n",
    "\n",
    "print(np.array([12, 5, 2])) # Vektor\n",
    "x = np.array([\n",
    "    [1, 2, 4],\n",
    "    [5, 2, 5],\n",
    "    [6, 9, 2]\n",
    "]).astype('float32')\n",
    "print(x) # Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Manipulation von NumPy-Datentypen</h1><br>\n",
    "Tensoren haben folgende wichtige Eigenschaften:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3, 3)\n",
      "float32\n",
      "2\n",
      "(60000, 784)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(x.ndim) # Zahl der Achsen\n",
    "print(x.shape) # wie viele Einheiten entlang jeder Achse: hier 3 Zeilen, 3 Spalten\n",
    "print(x.dtype) # der NumPy-Datentyp\n",
    "\n",
    "print(train_images.ndim)\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Auswählen von Daten aus einem Tensor</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 784)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-16f84b2b6374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmySlice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ist das gleiche wie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmySlice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmySlice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "mySlice = train_images[10:100] # Auswahl Beobachtungen 1 bis 99\n",
    "print(mySlice.shape)\n",
    "# ist das gleiche wie\n",
    "mySlice = train_images[10:100, 0:28, 0:28]\n",
    "print(mySlice.shape)\n",
    "\n",
    "# Auswahl der unteren rechten Ecke ALLER Bilder:\n",
    "mySlice = train_images[:, 14:, 14:] # alle Bilder, Px 14 und darüber, px 14 und darüber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Batches von Daten</b><br>\n",
    "In allen Datensätzen ist der nullte Dimension normalerweise die Dimension der jeweiligen Beobachtung. Diese wird beim Tranining des Modells in Batches aufgeteilt, hier in Batches von 128 Beobachtungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = train_images[:128] # erster Batch Groesse 128 von 0 bis 127\n",
    "batch = train_images[128:256] # zweiter 128 ... 255\n",
    "# batch = train_images[128 * n:128 * (n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tensoroperationen</b><br>\n",
    "Neuronale Netzwerke können auf eine Reihe von Tensoroperationen (bspw. Matrixmultiplikation) reduziert werden, die folgend erklärt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "exampleLayer= layers.Dense(512, activation = 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Schicht kann als eine Funktion interpretiert werden, die einen 2D-Tensor als Input nimmt und ebenso einen 2D-Tensor wieder ausgibt. \n",
    "$$output = relu(dot(W, input) + bias)$$ mit $$relu(x) = max \\{x, 0\\}$$\n",
    "All die Operationen sind Operationen, die auf allen Elementen eines Tensors ausgeführt wird: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  5. -1.]\n",
      "[0. 5. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1, 2, 3]).astype('float32')\n",
    "y = np.array([-2, 3, -4]).astype('float32')\n",
    "z = x + y # elementweise Addition\n",
    "print(z)\n",
    "print(np.maximum(z, 0.)) # elementweise relu-Funktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All diese Operationen auf einzelnen Elementen sind durch Basic Linear Algebra Subprograms (BLAS) unglaublich schnell. Das sind Subroutinen, geschrieben in Fortran oder C, die installiert werden sollten, um all diese Opeartionen schneller zu machen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Broadcasting</b><br>\n",
    "Es sollte für NumPy noch das Prinzip des Broadcasting erwähnt werden, bei dem Tensoren erweitert werden, sodass Operationen mit anderen Tensoren möglich sind, die ohne Erweiterung mathematisch nicht möglich wären. Man kann sich die Implementierung so vorstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2 # x ist eine Matrix, assert heisst: wenn nicht wahr, schmeisse Error\n",
    "    assert len(y.shape) == 1 # y ist ein Vektor\n",
    "    assert x.shape[1] == y.shape[0] # eine Spalte in x ist so lang wie der y-Vektor\n",
    "    x = x.copy() # überschreibe nicht die originale Variable\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j] # also zu jeder Spalte von x wird y addiert\n",
    "            # man kann sagen, y wird solange neben sich selbst gelegt, bis es so gross wie x ist und dann koennen sie addiert werden\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Matrix-Produkt kann folgendermaßen ausgedrückt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 2)\n",
      "[[14. 14.]\n",
      " [11. 15.]\n",
      " [10. 14.]]\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([\n",
    "    [1, 2, 3],\n",
    "    [2, 3, 1],\n",
    "    [3, 2, 1]\n",
    "]).astype('float32')\n",
    "\n",
    "y = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 2]\n",
    "]).astype('float32')\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "z = np.dot(x, y)\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Umformung von Tensoren</b><br>\n",
    "Das beudeutet, die Spalten und Zeilen eines Tensors so umzuformen, dass sie eine bestimmte Zielform erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(6, 1)\n",
      "[[1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]]\n",
      "[[1. 2. 2.]\n",
      " [3. 3. 2.]]\n",
      "[[1. 3.]\n",
      " [2. 3.]\n",
      " [2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 2]\n",
    "]).astype('float32')\n",
    "print(x.shape)\n",
    "\n",
    "x = x.reshape((6, 1))\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "x = x.reshape((2, 3))\n",
    "print(x)\n",
    "\n",
    "# besondere Form: transponieren\n",
    "print(np.transpose(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optimierung und eine mathematischere Sichtweise</h1><br>\n",
    "Für bereits erwähnt, kann jede Schicht eines NN ausgedrückt werden als <code>output = relu(dot(W, input) + bias)</code>. Dabei sind W und bias sogenannte trainierbare Parameter, sie repräsentieren das Wissen des NN und werden in einem Schritt genannt <i>zufällige Initialisierung</i> mit Werten belegt, die den Startpunkt für die Optimierung bieten. Nun werden diese Parameter langsam angepasst, abhängig von einem Feedback-Signal. Diese Phase wird das Training des NN genannt. Das passiert in der Training-Loop:\n",
    "1. Hole dir ein Batch von Input samples X und den dazugehörigen Labels y\n",
    "2. Gib X in das NN und erhalte die vorhergesagten Werte y_pred\n",
    "3. Errechne den Verlust, also den Unterschied von y_pred und y\n",
    "4. Aktualisiere die Parameter, sodass der Verlust ein wenig gemindert wird<br>\n",
    "\n",
    "Der schwierige Teil ist Teil 4: wie genau sollen die Gewichte angepasst werden? Alle Funktionen, die im NN genutzt werden sind differenzierbar.<br>\n",
    "Stell dir einen Input-Vektor <code>x</code>, eine Matrix <code>W</code>, Labels <code>y</code> und eine Loss-Funktion <code>loss</code> vor. Wir nutzen <code>W</code> und <code>x</code>, um <code>y_pred</code> zu errechnen: <code>y_pred = dot(W, x)</code>. Dann folgt der Vergleich mit den Labels: <code>loss_value = loss(y_pred, y)</code>.<br>\n",
    "Werden <code>x</code> und <code>y</code> nun eingefroren, kann das System als eine Funktion interpretiert, die die Gewichte auf den Verlust mappt: <code>loss_value = f(W)</code>. Nun wird der Gradient <code>gradient(f)(W)</code> in <code>W = W0</code> ermittelt. Sie stellt die Steigung der Loss-Function dar. Nun muss W mit einem kleinem inkrementellen Schritt <code>step</code> modifiziert werden (<code>W1 = W0 - step * gradient(f)(W0)</code>). Das heißt der Gradient in W0 wird mit dem negativen step multipliziert und dann von W0 abgezogen: <b>jedes Element in W wird ein wenig in die Richtung verändert, die den Loss fallen lässt</b>.\n",
    "<img src=\"./imgs/Loss.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Der Backpropagation Algorithmus</h1><br>\n",
    "Backpropagation beschreibt die Anpassung der Gewichte und Biases als Folge des Wertes der Verlustfunktion nach einem Trainingsbeispiel. Wie bereits erwähnt wollen wir die Gewichte des Netzwerks so verändern, dass sie auf ein (lokales) Minimum der Kostenfunktion zusteuern, also in Richtung $-\\nabla{C(w_{1}, ..., w_{n})}$, wobei die Kostenfunktion $C$ ein Funktion des Unteschieds zwischen dem Output des NN und dem richtigen Wert ist. Kleiner Einschub: der Gradient $\\nabla$ zeigt, in welche Richtung sich die Funktion nach oben bewegt, also zeigt $-\\nabla$, in welche Richtung sie sich nach unten bewegt. Der Wert zeigt also, in welche Richtung alle Gewichte gestoßen werden müssen, damit sich der Output dem Ziel angleicht. Der Backpropagation-Algorithmus errechnet diesen komplizierten Gradienten.<br><br>\n",
    "<b>Quelle für die Bilder aus dem folgenden Beispiel</b>: https://www.youtube.com/watch?v=Ilg3gGewQ5U.\n",
    "<img src=\"./imgs/example_NN_3blue1brown.png\">\n",
    "Wir haben ein untrainiertes Netzwerk, also ist der Unterschied zwischen dem Ziel \"2\" und der Outputschicht noch groß, wie im Bild zu sehen. <img src=\"./imgs/updown_3blue1brown.png\"> Wie bringen wir die Gewichte richtig dazu, dass das Outputwert, der 2 indiziert erhöht wird, und alle anderen gesenkt werden? Um das möglichst effizient zu tun, sollte die Größe der Veränderung proportional dazu sein, wie unterschiedlich sie zum gewünschten Ergebnis sind.<br>\n",
    "Wir wissen nun, wie sich der Output verändern sollte und wir können das tun, indem wir die Gewichte zu dieser Einheit verändern. Nehmen wir also Beispiel das Outputneuron für den Wert 2, dessen Wert wir erhöhen wollen. Zur Erinnerung, der Wert für die 2 entsteht aus ihren Gewichten mal der Aktivierung der Neuronen in der vorigen Schicht plus einem Bias, dann durch eine Aktivierungsfunktion $f$: $$a_{2}=f(w_{0,2}a_{0,2}+...+w_{n-1,2}a_{n-1,2}+b)$$<img src=\"./imgs/weightsTo2_3blue1brown.png\">\n",
    "Wie geben wir dem Outputneuron für die 2 ein stärkeres Signal?<br>\n",
    "Erstens, wir verändern die Gewichte zu diesem Neuron: wir verstärken jene Gewichte von Neuronen, die bei der zwei aktiviert worden. Wir verändern $w_i$ im Verhältnis zu $a_i$, da Veränderungen in stark aktivierten Neuronen der vorigen Schicht das Signal des 2-Outputneurons schneller erhöhen als weniger aktivierte. Gleichzeitig erniedrigen wir das Gewicht der wenig aktivierten Neuronen.<br>\n",
    "Zweitens, wir verändern die Aktivierungen der vorigen Schicht. Wenn alle Neuronen mit einem starken Gewicht zum 2-Neuron stärker werden und alle mit einem kleinen/negativen Gewicht schwächer, dann wird das 2-Neuron eher aktiviert werden. Wir ändern also $a_i$ im Verhältnis zu $w_i$.<br>\n",
    "Natürlich können wir die Aktivierung nicht direkt beeinflussen, da das vom Trainingsbeispiel abhängt, <b>aber</b> wir wissen, welche Veränderung in der Aktivierung des Neurons wir uns wünschen. Somit können wir die Gewichte dieses Neurons in der vorigen Schicht anpassen, damit uns die Aktivierung für Schicht vor dem 2-Neuron für das 2-Neuron gelegen kommt. <b>Merke</b>: dieser Prozess setzt sich also bis zu den Gewichten von den Inputneuronen fort: es erfoglte eine <i>backpropagation</i> vom Unterschied im Output bis zu den Gewichten der Inputneuronen.<br>\n",
    "Außerdem ist es wichtig, dass all das für die Veränderung der Neuron für die Änderung <i>eines</i> Outputneurons ist. Alle Outputneuronen melden also über Backpropagation ein gewünschte Änderung, die aufaddiert wird. <i>Und das alles nur für ein Trainingsbeispiel!</i><img src=\"./imgs/alleNeuronen_3blue1brown.png\"><br><br>\n",
    "<b>Batching</b><br>\n",
    "Das Trainieren eines Batches (= einem Teil des Datensatzes) ist gleichzusetzen mit dem Errechnen des Outputs und der Veränderung der Gewichte. Also werden die Outputs für einen Batch errechnet, und anschließend die Gewichte gemäß des \"durchschnittlichen Wunsches\" zur Veränderung und der damit verbundenen geänderten Position auf $C$.<br>\n",
    "Je größer der Batch, desto genauer der Schritt in Richtung eines Minimus auf $C$, weil es den Gesamtdatensatz besser repräsentiert. Allerdings wird die Berechnung länger dauern und mehr RAM brauchen.\n",
    "Betrachtet man die beiden Extreme, ein Batch mit je einem Beispiel und ein Batch mit allen Trainingsbeispielen, dann \"wandert\" NN mit kleineren Batchgrößen zu ziellos auf $C$ herum, während eine zu große Batch-Größe nicht mehr aus einem lokalen Minimum auf $C$ herausfindet.<br>\n",
    "<b>Kurz: je größer der Batch, desto genauer die Schritte auf $C$, aber desto mehr Rechenkraft benötigt.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
