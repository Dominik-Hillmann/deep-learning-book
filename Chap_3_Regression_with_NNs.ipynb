{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regressionsprobleme mit Neuronalen Netzen</h1><br>\n",
    "Hier warten nun ganz andere Probleme, z.B. unterschiedliche Skalierung der Inputvariablen.<br><br><b>Der Boston Housing Price-Datensatz</b><br>\n",
    "Dieser ist recht klein, 506 Beobachtungen, wobei jedes Feature eine andere Skalierung hat. Die Ziele sind die die Medianwerte der Häuser in Tsd. Dollar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "[  0.67191   0.        8.14      0.        0.538     5.813    90.3\n",
      "   4.682     4.      307.       21.      376.88     14.81   ] \n",
      " 16.6\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "print(train_data.shape) # 404 Beobachtungen mit 13 Variablen\n",
    "print(train_data[28], '\\n',train_targets[28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten müssen bei unterschiedlicher Skalierung nicht präpariert werden, indem sie One-Hot-kodiert werden. Hier müssen sie normalisiert werden. Wie bei der Hauptkomponentenanalyse. Dafür gibt es in np-Arrays eingebaute Methoden, die mithilfe von axis = 0 die Statistiken für das jeweilige Feature/die jeweilige Variable finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.74511057e+00 1.14801980e+01 1.11044307e+01 6.18811881e-02\n",
      " 5.57355941e-01 6.26708168e+00 6.90106436e+01 3.74027079e+00\n",
      " 9.44059406e+00 4.05898515e+02 1.84759901e+01 3.54783168e+02\n",
      " 1.27408168e+01]\n",
      "[  0.67191   0.        8.14      0.        0.538     5.813    90.3\n",
      "   4.682     4.      307.       21.      376.88     14.81   ]\n",
      "[-0.3329834  -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.64053626\n",
      "  0.76289357  0.4644321  -0.62624905 -0.59517003  1.14850044  0.23508618\n",
      "  0.28557943]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.mean(axis = 0))\n",
    "print(train_data[28])\n",
    "\n",
    "means = train_data.mean(axis = 0)\n",
    "std_deviations = train_data.std(axis = 0)\n",
    "train_data -= means\n",
    "train_data /= std_deviations\n",
    "\n",
    "print(train_data[28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Problem mit kleinen Datensätzen wie diesem hier, dass sie leicht zum Overfitting führen. Um dem etwas entgegenzusetzen, werden relativ kleine Netze verwendet. Das sollte verhindern, dass sich ein zu großes Netz auf die nicht-generalisierbaren Eigenheiten der Trainingsdaten einpendeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    # will have multiple instances, that's why we use a function\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(\n",
    "        64, activation = 'relu', \n",
    "        input_shape = (train_data.shape[1], )\n",
    "    ))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = 'rmsprop',\n",
    "        loss = 'mse',\n",
    "        metrics = ['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die letzte Schicht endet mit einem einzelnen Neuron und ohne Aktivierungsfunktion, ist also linear, da uns ja der uneingeschränkte Outputwert, also der vorausgesagte Wert des Hauses, interessiert. Würdest du z.B. die Sigmoid über das letzte Neuron legen, würden die linearen Werten auf [0,1] abgebildet werden. Ist nicht unser Ziel. Die Loss Function ist nun der MSE, Mean Squared Error, die quadrierte Differenz zwischen dem Ziel und dem vorausgesagten Wert. Der beobachtete Kennwert ist MAE, mean absolute error, also der nicht-quadrierte MSE, einfach nur die Abweichung Ziel <=> Voraussage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>K-Fold-Validation</b><br><br>\n",
    "Wenn Datensätze bereits klein sind, kann ein weiteres Aussieben der Validierungsdaten dem Modell nur schaden, weil das Ergebnis sehr davon abhängig ist, welche wenige Datenpunkte noch über sind.<br>\n",
    "Darum wird diese Technik genutzt. Die Daten werden in k Teile partitioniert. Es werden k Modelle trainiert auf k - 1 Teilen, wobei 1 Teil der Validierung dient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Processing fold 1\n",
      "303 101\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_15 to have shape (1,) but got array with shape (13,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-97eafaa6fa36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_15 to have shape (1,) but got array with shape (13,)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "print(int(len(train_data) / k))\n",
    "num_val_samples = int(len(train_data) / k)\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold ' + str(i + 1))\n",
    "    # Daten, von : bis\n",
    "    val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
    "    val_targets = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate([\n",
    "        train_data[:i * num_val_samples],      # alles vor Validierungsdaten\n",
    "        train_data[(i + 1) * num_val_samples:] # alles nach Validierungsdaten\n",
    "    ], axis = 0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate([\n",
    "        train_targets[:i * num_val_samples],      # alles vor Validierungsdaten\n",
    "        train_targets[(i + 1) * num_val_samples:] # alles nach Validierungsdaten\n",
    "    ], axis = 0)\n",
    "    \n",
    "    print(len(partial_train_data), len(val_data))\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(\n",
    "        np.array(partial_train_data),\n",
    "        np.array(partial_train_targets),\n",
    "        epochs = num_epochs,\n",
    "        batch_size = 1,\n",
    "        verbose = 0\n",
    "    )\n",
    "    \n",
    "    mse, mae = model.evaluate(np.array(val_data), np.array(val_targets), verbose = 0)\n",
    "    all_scores.append(mae)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
