{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortgeschrittene Konzepte für RNNs\n",
    "## GRU-Schichten\n",
    "GRU (Gated Recurrent Unit)-Schichten erfüllen diesselbe Funktion wie eine LSTM-Schicht: sie lassen den Fluss von Informationen aus weit vorangegangen Perioden gewichtet zu.\n",
    "\n",
    "Der Unterschied ist, dass sie weniger Rechenleistung benötigen, was mit einer kleineren Repäsentationskraft bezahlt wird\n",
    " \n",
    "## Recurrent Dropout\n",
    "In der Forschung schien sich zunächst abzubilden, dass Dropout in RNNs eher hinderlich als hilfreich ist. Wird allerdings das gleiche Muster an Dropout auf jede Schicht angewandt, so ändert sich das. So sollte auch das gleiche Muster in allen Matrizen innerhalb einer RNN-Schicht verwendet werden. Dies lässt sich so codieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 32)                3456      \n",
      "=================================================================\n",
      "Total params: 3,456\n",
      "Trainable params: 3,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU\n",
    "model = Sequential()\n",
    "# ...\n",
    "model.add(GRU(\n",
    "    32, \n",
    "    dropout = 0.2,\n",
    "    recurrent_dropout = 0.2,\n",
    "    input_shape = (None, data.shape[-1])\n",
    "))\n",
    "print(model.summary())\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mehrere RNN-Schichten hintereinander\n",
    "Um das zu tun, müssen alle Schichten, die nicht ganz oben liegen `return_sequences = True` aktiviert haben. Es ist ein guter Weg, um ein Netz zu erweitern, um es an die Grenzen des Overfitting zu bringen, und so mehr Repräsentationskraft zu finden.\n",
    "\n",
    "## In zwei Richtungen verlaufende RNNs\n",
    "Das sind Modelle, die gerne im Natural Language Processing verwendet werden und mehr Performanz bieten.\n",
    "\n",
    "In vielen Anwendungen, in denen die Reihenfolge von Daten von Bedeutung ist ex ante nicht klar, ob die Umkehr der Reihenfolge genauso gute Ergebnisse liefern kann. In Bereichen wie der Sprachverarbeitung funktioniert die Umkehrungen der Reihenfolge gut, denn die Bedeutung eines Wortes ist nicht nur abhängig von den vorangegangen Worten, sondern von der Position im gesamten Satz und somit auch von nachfolgenden Wörten. Aus diesem Grund kann das Umkehren der Sequenz sinnvoll sein und die sich daraus ergebenden Repräsentationen sollten genutzt werden.\n",
    "Diese Überlegungen liegen den *bidirectional RNNs* zugrunde, welche sich ihre Inputs aus zwei Richtungen ansehen und beide daraus entstehenden Repäsentationen einbeziehen.\n",
    "\n",
    "Technisch gesehen wird eine RNN-Schicht auf der chronologischen Reihenfolge trainiert und eine auf der antichronologischen Reihenfolge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, LSTM, GRU, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Bidirectional(GRU(32)))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andere Wege zur Steigerung der Performanz\n",
    "* Anpassen der Einheitenzahl in den recurrent Schichten\n",
    "* Anpassen der Lernrate des `RMSprop`-Optimierers\n",
    "* LSTM- statt GRU-Schichten\n",
    "* größere Schichten von `Dense`-Schichten\n",
    "* nicht vergessen, die besten Modelle auf dem Test-Set laufen zu lassen, da durch information leakage die Modelle auf dem Validierungssatz overfitten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
