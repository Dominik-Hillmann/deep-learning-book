{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Die Convolution und Pooling Operationen</h1><br>\n",
    "Convolutional Neural Networks dienen der Arbeit mit Bildern. Ihre Genauigkeit übertrifft denen normaler NNs. Das liegt daran, dass bevor die Bilder in dicht vernetzte Schichten (<code>Dense</code>) eingespeist werden, werden sie durch zwei andere Arten von Schichten geschickt.<br><br>\n",
    "<b>Die Convolution-Operation</b><br>\n",
    "Abstrakt gesagt ist der Unteschied zwischen normalen NNs und CNNs, dass CNNs lokale Muster lernen, während NNs sich auf gloable Muster beschränken müssen. Dabei lernen die Convolutional Schichten die lokalen Muster. Ist eine Muster an einer Stelle im Bild gelernt, kann es überall wiedererkannt werden. Mehrere Convolutional Schichten hintereinander können komplexere Muster im Bild erkennen und somit etwas anhand komplexer Konzepte im Bild erkennen. Das passiert, indem die einfachen Muster der ersten Convolution-Schicht, wie \"vertikaler Strich\" oder \"Strich von oben links nach unten rechts\", sich in darauffolgenden Schichten zu komplexeren Mustern zusammensetzen, wie \"Ohr\" oder \"Auge\" beim Erkennen menschlicher Gesichter.<br><br><img src=\"./imgs/cnn_cat.png\"><br><br>\n",
    "Der Input in ein CNN sind Bilder mit den Dimensionen <code>(height, width, channels)</code>. Die Channels beschreiben z.B. Farbanteile wie rot, grün, blau (RGB). Im Falle von RGB wären es also drei Channel. Im Falle eines schwarz-weißen Bildes wäre es 1, die Graustufen. Die Convolution extrahiert darauf kleinere Teile des Bildes, meist der Größe (3, 3) oder (5, 5). Über diese wird Pixel für Pixel ein Filter geschoben, der kleine Muster, wie z.B. \"querer Strich\" kodiert. Der Rückgabewert der Schicht ist dann ein Tensor der Dimension <code>(height - k, width - k, Anzahl Filter)</code>. k sind ein oder zwei Pixel am Rand, die nicht mit einbezogen werden können, weil die Filter sonst teilweise nicht im Bild liegen würden. Jeder Filter gibt eine sogenannte Response Map (<code>(height - k, width - k, 1)</code>) zurück, die zeigt, welche Teile des Bilder am ehesten dem Filter entsprechen.<br><br><img src=\"./imgs/cnn_filter.png\"><br><br>\n",
    "Dabei wird zuerst der kleine Teil extrahiert. Das \"filtern\" erfolgt über Matrixoperation mit einem Kernel. Der Kernel kodiert das Konzept, indem er die Bedeutung der Pixel um den Mittelpixel gewichtet. Die Berechnung erfolgt dann mittels der Matrixmultiplikation zwischen Ausschnitt und Kernel/Filter für jeden Kernel (Anzahl Kernel -> Output Depth), Dimension <code>(1, 1, Anzahl Filter)</code>. Dann wird der Ausschnitt untersucht, der sich einen Pixel weiter befindet, wiederholt, usw. Die 1 * 1 Ausschnitte werden dann woeder zur Feature Map zusammengesetzt für jeden der Kernel.<br><br><img src=\"./imgs/cnn_feature_maps.png\"><br><br>\n",
    "Will man eine Feature Map als Output, die die gleichen Dimensionen hat wie die des Inputs, kann man dies über das Padding steuern. Padding fügt dem Bild am Rand genügend Pixel hinzu, sodass der Ausschnitt niemals außerhalb des Bildes liegen kann. Außerdem beeinflusst das Striding die Größe des Outputs. Striding beschreibt, um wie viele Pixel der Ausschnitt springt. Allerdings wird selten ein Striding != 1 genutzt.<br><br>\n",
    "<b>Die Max-Pooling-Operation</b><br><br>\n",
    "Diese Schichten werden genutzt, um ein Bild aggressiv zu verkleinern. Ähnlich den Convolution-Schichten entziehen sie dem Bild kleinere Bilder. Meistens bestehen diese kleineren Bilder aus 2 * 2-Ausschnitten mit einem Stride von 2, also existiert keine Überlappung. Aus diesen Ausschnitten wird eine der Maximalwert entnommen und in die nächste Schicht eingespeist. Aber warum sollte das getan werden? Zum einen werden dadurch die Muster \"gepresst\", sodass Muster auf einem höheren Level gelernt werden können und zum anderen bedürften das Modell sonst zu viele Parameter, was zu Overfitting führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small vorhanden.\n",
      "Sets vorhanden.\n",
      "Unterteilungen bereits vorhanden.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aus Datensatz einen kleineren zum üben erstellen\n",
    "import os, shutil\n",
    "\n",
    "# create new directory for downsized dataset\n",
    "org_data_dir = '/home/dominik/Documents/Datasets/cats_and_doggos/train'\n",
    "small_data_dir = '/home/dominik/Documents/Datasets/cats_and_doggos/small'\n",
    "if not os.path.isdir(small_data_dir):\n",
    "    os.mkdir(small_data_dir)\n",
    "    print('Small noch nicht vorhanden.')\n",
    "else:\n",
    "    print('Small vorhanden.')\n",
    "\n",
    "# directories train, validation and test\n",
    "train_dir = os.path.join(small_data_dir, 'train')\n",
    "val_dir = os.path.join(small_data_dir, 'val')\n",
    "test_dir = os.path.join(small_data_dir, 'test')\n",
    "\n",
    "if not os.path.isdir(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(val_dir)\n",
    "    os.mkdir(test_dir)\n",
    "    print('Sets noch nicht vorhanden.')\n",
    "else:\n",
    "    print('Sets vorhanden.')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cat')\n",
    "val_cats_dir = os.path.join(val_dir, 'cat')\n",
    "test_cats_dir = os.path.join(test_dir, 'cat')\n",
    "\n",
    "train_dogs_dir = os.path.join(train_dir, 'dog')\n",
    "val_dogs_dir = os.path.join(val_dir, 'dog')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dog')\n",
    "\n",
    "if not os.path.isdir(train_cats_dir):\n",
    "    print('Unterteilungen noch nicht vorhanden.')\n",
    "    os.mkdir(train_cats_dir)\n",
    "    os.mkdir(val_cats_dir)\n",
    "    os.mkdir(test_cats_dir)\n",
    "\n",
    "    os.mkdir(train_dogs_dir)\n",
    "    os.mkdir(val_dogs_dir)\n",
    "    os.mkdir(test_dogs_dir)\n",
    "else:\n",
    "    print('Unterteilungen bereits vorhanden.')\n",
    "\n",
    "print('\\n')\n",
    "if not os.path.exists(os.path.join(train_cats_dir, 'cat.28.jpg')):\n",
    "    set_ranges = {\n",
    "        'train': range(1000),\n",
    "        'val': range(1000, 1500),\n",
    "        'test': range(1500, 2000)\n",
    "    }\n",
    "    \n",
    "    for animal in ['dog', 'cat']:\n",
    "        for set in ['train', 'val', 'test']:\n",
    "            \n",
    "            file_names = ['{0}.{1}.jpg'.format(animal, i) for i in set_ranges[set]]\n",
    "            print(file_names[28])\n",
    "            \n",
    "            for file_name in file_names:\n",
    "                src = os.path.join(org_data_dir, file_name)\n",
    "                dst = os.path.join(small_data_dir, set, animal, file_name)\n",
    "                shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vorbereitung der Daten</b><br>\n",
    "Die Bilder müssen vorher in die richtige Form umgewandelt werden, da das Netz nicht einfach so die JPGs aufnehmen kann. Dazu werden die Bilder gelesen, in RGB-float-Tensoren umgewandelt, und dann die Pixelwerte von 0 bis 255 auf 0 bis 1 umgewandelt, da NNs eher kleine Werte aufnehmen sollten. Diese Aufgaben übernimmt der <code>ImageDataGenerator</code>, der in Keras enthalten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "rescale_factor = 1.0 / 255.0 # sets values in [0,255] to [0,1]\n",
    "image_size = (150, 150) # will automatically resize images to these dimensions\n",
    "\n",
    "# constructor ImageDataGenerator, every iteration in for ... in gives batch of 20\n",
    "train_generator = ImageDataGenerator(rescale = rescale_factor).flow_from_directory(\n",
    "    train_dir, # target directory\n",
    "    target_size = image_size,\n",
    "    batch_size = 20,\n",
    "    class_mode = 'binary' # binary_crossentropy loss --> binary labels\n",
    "                          # binäre Labels werden anscheinend schon anhand der Ordner zugeordnet\n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator(rescale = rescale_factor).flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size = image_size,\n",
    "    batch_size = 20,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# schauen wir uns einen Batch an, der vom Generator erzeugt wird\n",
    "i = 0\n",
    "for (data_batch, label_batch) in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', label_batch)\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist zu sehen, dass der Tensor die Form <code>(20, 150, 150, 3)</code> hat. 20 Bilder, 150 mal 150 Pixel, rot, grün, blau. Die Labels werden automatisch aus den gegebenen Unterordnern generiert und binär benannt, darum das Argument 'binär'. Dieser Generator kann nun ewig Daten generieren, weshalb dem Modell gesagt werden muss, wie viele Bilder sich in dem Ordner befinden, um zu wissen, wann eine Epoche vorbei ist. Dazu gibt es <code>steps_per_epoch</code>. Hier mit 2000 Bildern und Batches von 20 brauchen wir also 100 Schritte, um einmal alle Bilder erhalten zu haben. Außerdem wird beim Trainieren des Modells nun <code>fit_generator()</code> genutzt. Was ist mit den Validierungsdaten? Diese können ein Tupel von np-Arrays sein, wie gehabt, oder ein Generator. Im Falle eines Generators muss dem Modell mittels <code>validation_steps</code> gesagt werden, wie viele Batches gezogen werden, damit eine Epoche erfüllt ist. Beachte, dass Validierungs- und Trainingsdaten schon lange getrennt worden sind. Auf zum Modell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "# man sieht, dass die Zahl der Filter weiter ansteigt, während die Bildgröße weiter reduziert wird\n",
    "# so wird in fast allen covnets vorgegangen\n",
    "model = models.Sequential()\n",
    "model.add(\n",
    "    layers.Conv2D(\n",
    "        32, # Anzahl Filter, dritte Outputdimesion\n",
    "        (3, 3), # Größe der Filter, die nach Mustern suchen\n",
    "        activation = 'relu',\n",
    "        input_shape = (150, 150, 3) # 150 mal 150, RGB-Werte\n",
    "    )\n",
    ")\n",
    "model.add(layers.MaxPooling2D((2, 2))) # Maximum aus 2 mal 2 Feldern im Bild, verkleinert Bild\n",
    "model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten()) # die reduzierten feature maps werden in ein 1D-Array gestampft für die Dense-Schichten\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid')) # binäre Klassifikation\n",
    "\n",
    "model.summary() # um zu sehen, wie sich die Dimension verändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 65s 652ms/step - loss: 0.6903 - acc: 0.5285 - val_loss: 0.6712 - val_acc: 0.5730\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 61s 614ms/step - loss: 0.6608 - acc: 0.5990 - val_loss: 0.6645 - val_acc: 0.5640\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 0.6207 - acc: 0.6610 - val_loss: 0.6159 - val_acc: 0.6520\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 61s 607ms/step - loss: 0.5733 - acc: 0.6950 - val_loss: 0.6045 - val_acc: 0.6620\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 60s 601ms/step - loss: 0.5368 - acc: 0.7285 - val_loss: 0.5919 - val_acc: 0.6780\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.5167 - acc: 0.7395 - val_loss: 0.5686 - val_acc: 0.6860\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.4921 - acc: 0.7615 - val_loss: 0.5723 - val_acc: 0.7040\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.4653 - acc: 0.7765 - val_loss: 0.5563 - val_acc: 0.6970\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 0.4448 - acc: 0.7825 - val_loss: 0.5916 - val_acc: 0.6970\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 62s 618ms/step - loss: 0.4051 - acc: 0.8160 - val_loss: 0.5414 - val_acc: 0.7320\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 60s 598ms/step - loss: 0.3835 - acc: 0.8255 - val_loss: 0.5718 - val_acc: 0.6990\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 0.3544 - acc: 0.8505 - val_loss: 0.6597 - val_acc: 0.6900\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 61s 607ms/step - loss: 0.3402 - acc: 0.8485 - val_loss: 0.5695 - val_acc: 0.7280\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 62s 617ms/step - loss: 0.3109 - acc: 0.8635 - val_loss: 0.5843 - val_acc: 0.7300\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 62s 618ms/step - loss: 0.2870 - acc: 0.8775 - val_loss: 0.5963 - val_acc: 0.7270\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 63s 634ms/step - loss: 0.2569 - acc: 0.8975 - val_loss: 0.6600 - val_acc: 0.7250\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 0.2441 - acc: 0.9080 - val_loss: 0.6795 - val_acc: 0.7130\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 63s 634ms/step - loss: 0.2137 - acc: 0.9170 - val_loss: 0.6565 - val_acc: 0.7250\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 64s 639ms/step - loss: 0.1963 - acc: 0.9245 - val_loss: 0.6866 - val_acc: 0.7180\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 61s 607ms/step - loss: 0.1734 - acc: 0.9385 - val_loss: 0.7474 - val_acc: 0.7200\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 64s 643ms/step - loss: 0.1474 - acc: 0.9480 - val_loss: 0.7631 - val_acc: 0.7210\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 61s 611ms/step - loss: 0.1357 - acc: 0.9475 - val_loss: 0.7745 - val_acc: 0.7300\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 62s 618ms/step - loss: 0.1118 - acc: 0.9605 - val_loss: 1.0806 - val_acc: 0.6740\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 0.1018 - acc: 0.9660 - val_loss: 0.8479 - val_acc: 0.7120\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 0.0871 - acc: 0.9720 - val_loss: 0.9073 - val_acc: 0.7320\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 63s 634ms/step - loss: 0.0665 - acc: 0.9865 - val_loss: 0.9630 - val_acc: 0.7180\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 65s 655ms/step - loss: 0.0597 - acc: 0.9850 - val_loss: 0.9979 - val_acc: 0.7060\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 63s 630ms/step - loss: 0.0534 - acc: 0.9845 - val_loss: 1.0061 - val_acc: 0.7110\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 63s 626ms/step - loss: 0.0471 - acc: 0.9830 - val_loss: 1.1776 - val_acc: 0.7200\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 62s 623ms/step - loss: 0.0409 - acc: 0.9895 - val_loss: 1.1291 - val_acc: 0.7180\n"
     ]
    }
   ],
   "source": [
    "# usual RMSprop optimizer, binary crossentropy als loss-Funktion\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizers.RMSprop(lr = 1e-4),\n",
    "    metrics = ['acc']\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 100,\n",
    "    epochs = 30,\n",
    "    validation_data = val_generator,\n",
    "    validation_steps = 50\n",
    ")\n",
    "model.save('cats_and_dogs_small.h5') # .h5, Format, um große Datenmengen zu speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "\n",
    "model = models.load_model('cats_and_dogs_small.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6bfddd8d760f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 30\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "print('Training accuracy in blue, validation accuracy in red, max validation accuracy in green')\n",
    "plt.plot(\n",
    "    epochs, train_acc, 'b',\n",
    "    epochs, val_acc, 'r',\n",
    "    epochs, [max(val_acc) for _ in range(num_epochs)], 'g-'\n",
    ")\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "print('Training loss in blue, validation loss in red, minimum validation loss in green')\n",
    "plt.plot(\n",
    "    epochs, train_loss, 'b',\n",
    "    epochs, val_loss, 'r',\n",
    "    epochs, [min(val_loss) for _ in range(num_epochs)], 'g-'\n",
    ")\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
