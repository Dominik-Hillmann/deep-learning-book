{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Die Convolution und Pooling Operationen</h1><br>\n",
    "Convolutional Neural Networks dienen der Arbeit mit Bildern. Ihre Genauigkeit übertrifft denen normaler NNs. Das liegt daran, dass bevor die Bilder in dicht vernetzte Schichten (<code>Dense</code>) eingespeist werden, werden sie durch zwei andere Arten von Schichten geschickt.<br><br>\n",
    "<b>Die Convolution-Operation</b><br>\n",
    "Abstrakt gesagt ist der Unteschied zwischen normalen NNs und CNNs, dass CNNs lokale Muster lernen, während NNs sich auf gloable Muster beschränken müssen. Dabei lernen die Convolutional Schichten die lokalen Muster. Ist eine Muster an einer Stelle im Bild gelernt, kann es überall wiedererkannt werden. Mehrere Convolutional Schichten hintereinander können komplexere Muster im Bild erkennen und somit etwas anhand komplexer Konzepte im Bild erkennen. Das passiert, indem die einfachen Muster der ersten Convolution-Schicht, wie \"vertikaler Strich\" oder \"Strich von oben links nach unten rechts\", sich in darauffolgenden Schichten zu komplexeren Mustern zusammensetzen, wie \"Ohr\" oder \"Auge\" beim Erkennen menschlicher Gesichter.<br><br><img src=\"./imgs/cnn_cat.png\"><br><br>\n",
    "Der Input in ein CNN sind Bilder mit den Dimensionen <code>(height, width, channels)</code>. Die Channels beschreiben z.B. Farbanteile wie rot, grün, blau (RGB). Im Falle von RGB wären es also drei Channel. Im Falle eines schwarz-weißen Bildes wäre es 1, die Graustufen. Die Convolution extrahiert darauf kleinere Teile des Bildes, meist der Größe (3, 3) oder (5, 5). Über diese wird Pixel für Pixel ein Filter geschoben, der kleine Muster, wie z.B. \"querer Strich\" kodiert. Der Rückgabewert der Schicht ist dann ein Tensor der Dimension <code>(height - k, width - k, Anzahl Filter)</code>. k sind ein oder zwei Pixel am Rand, die nicht mit einbezogen werden können, weil die Filter sonst teilweise nicht im Bild liegen würden. Jeder Filter gibt eine sogenannte Response Map (<code>(height - k, width - k, 1)</code>) zurück, die zeigt, welche Teile des Bilder am ehesten dem Filter entsprechen.<br><br><img src=\"./imgs/cnn_filter.png\"><br><br>\n",
    "Dabei wird zuerst der kleine Teil extrahiert. Das \"filtern\" erfolgt über Matrixoperation mit einem Kernel. Der Kernel kodiert das Konzept, indem er die Bedeutung der Pixel um den Mittelpixel gewichtet. Die Berechnung erfolgt dann mittels der Matrixmultiplikation zwischen Ausschnitt und Kernel/Filter für jeden Kernel (Anzahl Kernel -> Output Depth), Dimension <code>(1, 1, Anzahl Filter)</code>. Dann wird der Ausschnitt untersucht, der sich einen Pixel weiter befindet, wiederholt, usw. Die 1 * 1 Ausschnitte werden dann woeder zur Feature Map zusammengesetzt für jeden der Kernel.<br><br><img src=\"./imgs/cnn_feature_maps.png\"><br><br>\n",
    "Will man eine Feature Map als Output, die die gleichen Dimensionen hat wie die des Inputs, kann man dies über das Padding steuern. Padding fügt dem Bild am Rand genügend Pixel hinzu, sodass der Ausschnitt niemals außerhalb des Bildes liegen kann. Außerdem beeinflusst das Striding die Größe des Outputs. Striding beschreibt, um wie viele Pixel der Ausschnitt springt. Allerdings wird selten ein Striding != 1 genutzt.<br><br>\n",
    "<b>Die Max-Pooling-Operation</b><br><br>\n",
    "Diese Schichten werden genutzt, um ein Bild aggressiv zu verkleinern. Ähnlich den Convolution-Schichten entziehen sie dem Bild kleinere Bilder. Meistens bestehen diese kleineren Bilder aus 2 * 2-Ausschnitten mit einem Stride von 2, also existiert keine Überlappung. Aus diesen Ausschnitten wird eine der Maximalwert entnommen und in die nächste Schicht eingespeist. Aber warum sollte das getan werden? Zum einen werden dadurch die Muster \"gepresst\", sodass Muster auf einem höheren Level gelernt werden können und zum anderen bedürften das Modell sonst zu viele Parameter, was zu Overfitting führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small noch nicht vorhanden.\n",
      "Sets noch nicht vorhanden.\n",
      "Unterteilungen noch nicht vorhanden.\n",
      "\n",
      "\n",
      "dog.28.jpg\n",
      "dog.1028.jpg\n",
      "dog.1528.jpg\n",
      "cat.28.jpg\n",
      "cat.1028.jpg\n",
      "cat.1528.jpg\n"
     ]
    }
   ],
   "source": [
    "# Aus Datensatz einen kleineren zum üben erstellen\n",
    "import os, shutil\n",
    "\n",
    "# create new directory for downsized dataset\n",
    "org_data_dir = '/home/dominik/Documents/Datasets/cats_and_doggos/train'\n",
    "small_data_dir = '/home/dominik/Documents/Datasets/cats_and_doggos/small'\n",
    "if not os.path.isdir(small_data_dir):\n",
    "    os.mkdir(small_data_dir)\n",
    "    print('Small noch nicht vorhanden.')\n",
    "else:\n",
    "    print('Small vorhanden.')\n",
    "\n",
    "# directories train, validation and test\n",
    "train_dir = os.path.join(small_data_dir, 'train')\n",
    "val_dir = os.path.join(small_data_dir, 'val')\n",
    "test_dir = os.path.join(small_data_dir, 'test')\n",
    "\n",
    "if not os.path.isdir(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(val_dir)\n",
    "    os.mkdir(test_dir)\n",
    "    print('Sets noch nicht vorhanden.')\n",
    "else:\n",
    "    print('Sets vorhanden.')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cat')\n",
    "val_cats_dir = os.path.join(val_dir, 'cat')\n",
    "test_cats_dir = os.path.join(test_dir, 'cat')\n",
    "\n",
    "train_dogs_dir = os.path.join(train_dir, 'dog')\n",
    "val_dogs_dir = os.path.join(val_dir, 'dog')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dog')\n",
    "\n",
    "if not os.path.isdir(train_cats_dir):\n",
    "    print('Unterteilungen noch nicht vorhanden.')\n",
    "    os.mkdir(train_cats_dir)\n",
    "    os.mkdir(val_cats_dir)\n",
    "    os.mkdir(test_cats_dir)\n",
    "\n",
    "    os.mkdir(train_dogs_dir)\n",
    "    os.mkdir(val_dogs_dir)\n",
    "    os.mkdir(test_dogs_dir)\n",
    "else:\n",
    "    print('Unterteilungen bereits vorhanden.')\n",
    "\n",
    "print('\\n')\n",
    "if not os.path.exists(os.path.join(train_cats_dir, 'cat.28.jpg')):\n",
    "    set_ranges = {\n",
    "        'train': range(1000),\n",
    "        'val': range(1000, 1500),\n",
    "        'test': range(1500, 2000)\n",
    "    }\n",
    "    \n",
    "    for animal in ['dog', 'cat']:\n",
    "        for set in ['train', 'val', 'test']:\n",
    "            \n",
    "            file_names = ['{0}.{1}.jpg'.format(animal, i) for i in set_ranges[set]]\n",
    "            print(file_names[28])\n",
    "            \n",
    "            for file_name in file_names:\n",
    "                src = os.path.join(org_data_dir, file_name)\n",
    "                dst = os.path.join(small_data_dir, set, animal, file_name)\n",
    "                shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vorbereitung der Daten</b><br>\n",
    "Die Bilder müssen vorher in die richtige Form umgewandelt werden, da das Netz nicht einfach so die JPGs aufnehmen kann. Dazu werden die Bilder gelesen, in RGB-float-Tensoren umgewandelt, und dann die Pixelwerte von 0 bis 255 auf 0 bis 1 umgewandelt, da NNs eher kleine Werte aufnehmen sollten. Diese Aufgaben übernimmt der <code>ImageDataGenerator</code>, der in Keras enthalten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "rescale_factor = 1.0 / 255.0 # sets values in [0,255] to [0,1]\n",
    "image_size = (150, 150) # will automatically resize images to these dimensions\n",
    "\n",
    "# constructor ImageDataGenerator, every iteration in for ... in gives batch of 20\n",
    "train_generator = ImageDataGenerator(rescale = rescale_factor).flow_from_directory(\n",
    "    train_dir, # target directory\n",
    "    target_size = image_size,\n",
    "    batch_size = 20,\n",
    "    class_mode = 'binary' # binary_crossentropy loss --> binary labels\n",
    "                          # binäre Labels werden anscheinend schon anhand der Ordner zugeordnet\n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator(rescale = rescale_factor).flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size = image_size,\n",
    "    batch_size = 20,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# schauen wir uns einen Batch an, der vom Generator erzeugt wird\n",
    "i = 0\n",
    "for (data_batch, label_batch) in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', label_batch)\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist zu sehen, dass der Tensor die Form <code>(20, 150, 150, 3)</code> hat. 20 Bilder, 150 mal 150 Pixel, rot, grün, blau. Die Labels werden automatisch aus den gegebenen Unterordnern generiert und binär benannt, darum das Argument 'binär'. Dieser Generator kann nun ewig Daten generieren, weshalb dem Modell gesagt werden muss, wie viele Bilder sich in dem Ordner befinden, um zu wissen, wann eine Epoche vorbei ist. Dazu gibt es <code>steps_per_epoch</code>. Hier mit 2000 Bildern und Batches von 20 brauchen wir also 100 Schritte, um einmal alle Bilder erhalten zu haben. Außerdem wird beim Trainieren des Modells nun <code>fit_generator()</code> genutzt. Was ist mit den Validierungsdaten? Diese können ein Tupel von np-Arrays sein, wie gehabt, oder ein Generator. Im Falle eines Generators muss dem Modell mittels <code>validation_steps</code> gesagt werden, wie viele Batches gezogen werden, damit eine Epoche erfüllt ist. Beachte, dass Validierungs- und Trainingsdaten schon lange getrennt worden sind. Auf zum Modell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "# man sieht, dass die Zahl der Filter weiter ansteigt, während die Bildgröße weiter reduziert wird\n",
    "# so wird in fast allen covnets vorgegangen\n",
    "model = models.Sequential()\n",
    "model.add(\n",
    "    layers.Conv2D(\n",
    "        32, # Anzahl Filter, dritte Outputdimesion\n",
    "        (3, 3), # Größe der Filter, die nach Mustern suchen\n",
    "        activation = 'relu',\n",
    "        input_shape = (150, 150, 3) # 150 mal 150, RGB-Werte\n",
    "    )\n",
    ")\n",
    "model.add(layers.MaxPooling2D((2, 2))) # Maximum aus 2 mal 2 Feldern im Bild, verkleinert Bild\n",
    "model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten()) # die reduzierten feature maps werden in ein 1D-Array gestampft für die Dense-Schichten\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid')) # binäre Klassifikation\n",
    "\n",
    "model.summary() # um zu sehen, wie sich die Dimension verändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 0.6916 - acc: 0.5310 - val_loss: 0.6735 - val_acc: 0.5570\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.6522 - acc: 0.6215 - val_loss: 0.6749 - val_acc: 0.5810\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 61s 607ms/step - loss: 0.5957 - acc: 0.6795 - val_loss: 0.6171 - val_acc: 0.6520\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 0.5633 - acc: 0.7025 - val_loss: 0.5976 - val_acc: 0.6690\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.5389 - acc: 0.7200 - val_loss: 0.6485 - val_acc: 0.6350\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 60s 600ms/step - loss: 0.5183 - acc: 0.7415 - val_loss: 0.6582 - val_acc: 0.6430\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.4958 - acc: 0.7540 - val_loss: 0.5761 - val_acc: 0.6960\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 60s 605ms/step - loss: 0.4646 - acc: 0.7765 - val_loss: 0.5623 - val_acc: 0.7210\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 0.4374 - acc: 0.7975 - val_loss: 0.6643 - val_acc: 0.6710\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 63s 629ms/step - loss: 0.4157 - acc: 0.8025 - val_loss: 0.6020 - val_acc: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.3978 - acc: 0.8265 - val_loss: 0.5572 - val_acc: 0.7240\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.3602 - acc: 0.8440 - val_loss: 0.6724 - val_acc: 0.6830\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.3382 - acc: 0.8570 - val_loss: 0.5829 - val_acc: 0.7150\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 60s 601ms/step - loss: 0.3179 - acc: 0.8645 - val_loss: 0.5798 - val_acc: 0.7320\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 0.2941 - acc: 0.8710 - val_loss: 0.5989 - val_acc: 0.7310\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 0.2722 - acc: 0.8860 - val_loss: 0.6615 - val_acc: 0.7210\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 0.2465 - acc: 0.8990 - val_loss: 0.6188 - val_acc: 0.7390\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 61s 614ms/step - loss: 0.2258 - acc: 0.9060 - val_loss: 0.5997 - val_acc: 0.7340\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 63s 626ms/step - loss: 0.2006 - acc: 0.9250 - val_loss: 0.6561 - val_acc: 0.7260\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 64s 641ms/step - loss: 0.1741 - acc: 0.9385 - val_loss: 0.7241 - val_acc: 0.7090\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 0.1638 - acc: 0.9365 - val_loss: 0.7134 - val_acc: 0.7220\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 63s 628ms/step - loss: 0.1425 - acc: 0.9510 - val_loss: 0.7227 - val_acc: 0.7350\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 63s 629ms/step - loss: 0.1335 - acc: 0.9510 - val_loss: 0.7396 - val_acc: 0.7290\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 64s 640ms/step - loss: 0.1051 - acc: 0.9665 - val_loss: 1.0649 - val_acc: 0.7130\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 67s 672ms/step - loss: 0.0970 - acc: 0.9690 - val_loss: 0.8741 - val_acc: 0.7210\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 62s 625ms/step - loss: 0.0842 - acc: 0.9750 - val_loss: 0.8436 - val_acc: 0.7340\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 0.0738 - acc: 0.9775 - val_loss: 0.8678 - val_acc: 0.7350\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 65s 652ms/step - loss: 0.0511 - acc: 0.9890 - val_loss: 1.0807 - val_acc: 0.6960\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 63s 632ms/step - loss: 0.0530 - acc: 0.9865 - val_loss: 0.9551 - val_acc: 0.7280\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 63s 627ms/step - loss: 0.0386 - acc: 0.9885 - val_loss: 1.2344 - val_acc: 0.7120\n"
     ]
    }
   ],
   "source": [
    "# usual RMSprop optimizer, binary crossentropy als loss-Funktion\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizers.RMSprop(lr = 1e-4),\n",
    "    metrics = ['acc']\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 100,\n",
    "    epochs = 30,\n",
    "    validation_data = val_generator,\n",
    "    validation_steps = 50\n",
    ")\n",
    "model.save('cats_and_dogs_small.h5') # .h5, Format, um große Datenmengen zu speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = models.load_model('cats_and_dogs_small.h5')\n",
    "model2.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
